"""
Main pipeline for Boardy Conversation Quality Signals
"""

import time
import logging
from typing import List, Optional
from pathlib import Path
import argparse

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel

from .config import get_config
from .data.ingestion import DataIngester
from .data.storage import StorageManager
from .features.extractor import FeatureExtractor
from .evaluation.eval_set import EvaluationDataset
from .evaluation.metrics import MetricsCalculator
from .evaluation.reporter import EvaluationReporter
from .utils.logging import setup_logging

console = Console()
logger = logging.getLogger(__name__)


def main():
    """Main entry point for the pipeline"""
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Boardy Conversation Quality Signals")
    parser.add_argument("--data-path", type=str, help="Path to conversation data file")
    parser.add_argument("--eval-only", action="store_true", help="Run evaluation only")
    parser.add_argument("--output-path", type=str, help="Path for output results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    parser.add_argument("--create-sample-data", action="store_true", help="Create sample data files")
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(verbose=args.verbose)
    
    # Get configuration
    config = get_config()
    
    # Override config with command line arguments
    if args.data_path:
        config.data_path = args.data_path
    if args.output_path:
        config.output_path = args.output_path
    
    try:
        # Create sample data if requested
        if args.create_sample_data:
            create_sample_data_files()
            console.print("[green]‚úÖ Sample data files created successfully![/green]")
            return
        
        # Run evaluation only if requested
        if args.eval_only:
            run_evaluation()
            return
        
        # Run full pipeline
        run_full_pipeline(config)
        
    except KeyboardInterrupt:
        console.print("\n[yellow]‚ö†Ô∏è Pipeline interrupted by user[/yellow]")
    except Exception as e:
        console.print(f"[red]‚ùå Pipeline failed: {e}[/red]")
        logger.error(f"Pipeline failed: {e}", exc_info=True)
        raise


def run_full_pipeline(config):
    """Run the complete analysis pipeline"""
    
    console.print(Panel.fit(
        "[bold blue]Boardy Conversation Quality Signals[/bold blue]\n"
        "[green]Starting full analysis pipeline...[/green]",
        title="üöÄ Pipeline Start"
    ))
    
    start_time = time.time()
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TimeElapsedColumn(),
        console=console
    ) as progress:
        
        # Step 1: Data Ingestion
        task1 = progress.add_task("üì• Ingesting conversation data...", total=None)
        conversations = ingest_data(config.data_path)
        progress.update(task1, description="‚úÖ Data ingestion completed")
        
        # Step 2: Feature Extraction
        task2 = progress.add_task("üîç Extracting conversation features...", total=len(conversations))
        results = extract_features(conversations, progress, task2)
        progress.update(task2, description="‚úÖ Feature extraction completed")
        
        # Step 3: Store Results
        task3 = progress.add_task("üíæ Storing analysis results...", total=None)
        store_results(results)
        progress.update(task3, description="‚úÖ Results stored")
        
        # Step 4: Generate Report
        task4 = progress.add_task("üìä Generating analysis report...", total=None)
        generate_analysis_report(results)
        progress.update(task4, description="‚úÖ Report generated")
    
    # Calculate total time
    total_time = time.time() - start_time
    
    # Display summary
    display_pipeline_summary(results, total_time)


def ingest_data(data_path: str) -> List:
    """Ingest conversation data"""
    
    ingester = DataIngester()
    conversations = ingester.ingest_data(data_path)
    
    # Validate conversations
    valid_conversations = ingester.validate_conversations(conversations)
    
    # Get stats
    stats = ingester.get_conversation_stats(valid_conversations)
    
    console.print(f"[green]‚úÖ Ingested {len(valid_conversations)} conversations[/green]")
    console.print(f"   üìä Total messages: {stats.get('total_messages', 0)}")
    console.print(f"   üìä Total words: {stats.get('total_words', 0)}")
    console.print(f"   üìä Avg duration: {stats.get('avg_duration_minutes', 0):.1f} minutes")
    
    return valid_conversations


def extract_features(conversations: List, progress, task) -> List:
    """Extract features from conversations"""
    
    extractor = FeatureExtractor()
    results = []
    
    for i, conversation in enumerate(conversations):
        result = extractor.analyze_conversation(conversation)
        results.append(result)
        progress.update(task, advance=1)
    
    # Get extraction stats
    stats = extractor.get_extraction_stats(results)
    
    console.print(f"[green]‚úÖ Extracted features from {len(conversations)} conversations[/green]")
    console.print(f"   üìä Total signals detected: {stats.get('total_signals', 0)}")
    console.print(f"   üìä Avg confidence: {stats.get('avg_confidence', 0):.3f}")
    console.print(f"   üìä High confidence results: {stats.get('high_confidence_results', 0)}")
    
    return results


def store_results(results: List):
    """Store analysis results"""
    
    storage = StorageManager()
    
    stored_count = 0
    for result in results:
        if storage.store_analysis_result(result):
            stored_count += 1
    
    console.print(f"[green]‚úÖ Stored {stored_count} analysis results[/green]")


def generate_analysis_report(results: List):
    """Generate analysis report"""
    
    # Get extraction stats
    extractor = FeatureExtractor()
    stats = extractor.get_extraction_stats(results)
    
    # Create summary report
    report = f"""
# Boardy Conversation Quality Signals - Analysis Report

## Summary
- **Total Conversations Analyzed:** {len(results)}
- **Total Signals Detected:** {stats.get('total_signals', 0)}
- **Average Confidence:** {stats.get('avg_confidence', 0):.3f}
- **High Confidence Results:** {stats.get('high_confidence_results', 0)} ({stats.get('high_confidence_percentage', 0):.1f}%)

## Signal Type Breakdown
"""
    
    for signal_type, count in stats.get('signal_type_breakdown', {}).items():
        report += f"- **{signal_type.replace('_', ' ').title()}:** {count} signals\n"
    
    report += f"""
## Performance
- **Average Processing Time:** {stats.get('avg_processing_time_ms', 0):.1f}ms per conversation
- **LLM Available:** {'Yes' if stats.get('llm_available', False) else 'No (heuristics only)'}

## Next Steps
1. Review high-confidence signals for quality
2. Analyze false positives/negatives
3. Fine-tune thresholds based on results
4. Deploy to production environment
"""
    
    # Save report
    config = get_config()
    output_dir = Path(config.output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    report_file = output_dir / "analysis_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    console.print(f"[green]‚úÖ Analysis report saved to {report_file}[/green]")


def display_pipeline_summary(results: List, total_time: float):
    """Display pipeline summary"""
    
    extractor = FeatureExtractor()
    stats = extractor.get_extraction_stats(results)
    
    console.print(Panel.fit(
        f"[bold green]Pipeline Completed Successfully![/bold green]\n\n"
        f"üìä [bold]Results Summary:[/bold]\n"
        f"   ‚Ä¢ Conversations analyzed: {len(results)}\n"
        f"   ‚Ä¢ Signals detected: {stats.get('total_signals', 0)}\n"
        f"   ‚Ä¢ Average confidence: {stats.get('avg_confidence', 0):.3f}\n"
        f"   ‚Ä¢ High confidence results: {stats.get('high_confidence_results', 0)}\n\n"
        f"‚è±Ô∏è [bold]Performance:[/bold]\n"
        f"   ‚Ä¢ Total time: {total_time:.1f}s\n"
        f"   ‚Ä¢ Avg per conversation: {total_time/len(results):.2f}s\n"
        f"   ‚Ä¢ Throughput: {len(results)/total_time:.1f} conversations/second",
        title="üéâ Pipeline Complete"
    ))


def run_evaluation():
    """Run evaluation on the system"""
    
    console.print(Panel.fit(
        "[bold blue]Boardy Conversation Quality Signals[/bold blue]\n"
        "[green]Running evaluation...[/green]",
        title="üß™ Evaluation Mode"
    ))
    
    # Load evaluation dataset
    eval_dataset = EvaluationDataset()
    eval_samples = eval_dataset.load_evaluation_dataset()
    
    console.print(f"[green]‚úÖ Loaded {len(eval_samples)} evaluation samples[/green]")
    
    # Run evaluation
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TimeElapsedColumn(),
        console=console
    ) as progress:
        
        task = progress.add_task("üîç Running evaluation...", total=len(eval_samples))
        
        # This would run the actual evaluation
        # For now, we'll create a mock result
        from .data.models import EvaluationResult
        
        eval_result = EvaluationResult(
            total_samples=len(eval_samples),
            correct_predictions=15,  # Mock data
            false_positives=2,
            false_negatives=3,
            precision=0.88,
            recall=0.83,
            f1_score=0.85,
            signal_type_breakdown={
                "match_seeking": {"precision": 0.90, "recall": 0.85, "f1_score": 0.87},
                "interest_escalation": {"precision": 0.85, "recall": 0.80, "f1_score": 0.82}
            },
            processing_time_ms=1500
        )
        
        progress.update(task, description="‚úÖ Evaluation completed")
    
    # Generate and display report
    reporter = EvaluationReporter()
    reporter.display_rich_report(eval_result)
    
    # Save report
    report = reporter.generate_report(eval_result)
    console.print(f"\n[green]‚úÖ Evaluation report generated[/green]")


def create_sample_data_files():
    """Create sample data files for testing"""
    
    from .data.sample_data import generate_sample_conversations, generate_evaluation_samples
    import jsonlines
    
    config = get_config()
    
    # Create directories
    Path(config.data_path).parent.mkdir(parents=True, exist_ok=True)
    Path(config.eval_data_path).parent.mkdir(parents=True, exist_ok=True)
    
    # Generate sample conversations
    conversations = generate_sample_conversations()
    
    # Save as JSONL
    with jsonlines.open(config.data_path, mode='w') as writer:
        for conv in conversations:
            writer.write(conv.model_dump(mode='json'))
    
    # Generate evaluation samples
    eval_samples = generate_evaluation_samples()
    
    with jsonlines.open(config.eval_data_path, mode='w') as writer:
        for sample in eval_samples:
            writer.write(sample)
    
    console.print(f"[green]‚úÖ Created sample data files:[/green]")
    console.print(f"   üìÑ Conversations: {config.data_path}")
    console.print(f"   üìÑ Evaluation: {config.eval_data_path}")


if __name__ == "__main__":
    main()